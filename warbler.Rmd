---
title: 'IOFF: worked example'
author: "Lea Dambly"
date: "`r Sys.Date()`"
output: html_document
---

## Introduction

This is an example of model-based data integration for the IOFF paper using data for the black-throated blue warbler (*Setophaga caerulescens*) in Pennsylvania (from here on 'PA').

The datasets are:

* The North American Breeding Bird survey (BBS)
* eBird
* PA Breeding Bird Atlas (BBA) - a subset of an area within PA used by Miller et al.

The model covariates are:

* Elevation (from the USGS)
* Canopy cover (USGS National Land Cover Database)
* Cencus via censusapi

```{r libraries, message = FALSE}
library(plyr)
library(dplyr)
library(rBBS)
library(USAboundaries)
library(sf)
library(sp)
library(readr)
library(raster)
library(rgdal)
library(elevatr)
library(FedData)
library(RColorBrewer)
library(PointedSDMs)
library(mapview)
library(censusapi)
library(concaveman)
```

## Data - decide between full area or subset - DON'T run both

```{r data gathering - FULL AREA}
proj <- CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

PA <- us_states(states = "Pennsylvania")
PA <- PA$geometry[1]
PA <- as(PA, 'Spatial')

# BBA from Miller et al. appendix
load("PA_BBA.RData")

# sum counts (counts are undertaken in 5 time intervals at a single spatial point)
BBA_Wren <- bba %>%
  mutate(total = dplyr::select(., v1:v5) %>% rowSums(na.rm = TRUE)) %>%
  dplyr::select(-c(v1:v5)) %>%
  mutate(present = if_else(total == 0, FALSE, TRUE)) %>%
  rename(X = Longitude, Y = Latitude)

# change to spatial dataframe
BBA_sp <- SpatialPointsDataFrame(BBA_Wren[,c("X", "Y")],
                               data = BBA_Wren[,c("present", "point")],
                               proj4string = crs(proj))

# BBS (using rBBS package)
RegionMetaData <- GetRegions()
WeatherMetaData <- GetWeather()
# GetRoutes function has an issue. It calls Routes.zip instead of routes.zip
GetRoutes <- function(Dir="ftp://ftpext.usgs.gov/pub/er/md/laurel/BBS/DataFiles/") {
  routes=GetUnzip(ZipName=paste0(Dir, "routes.zip"), FileName="routes.csv")
  routes$routeID=paste(routes$statenum, routes$Route)
  routes
}
RoutesMetaData <- GetRoutes()

PACode <- RegionMetaData$RegionCode[RegionMetaData$`State/Prov/TerrName` == "PENNSYLVANIA"]
PAYears <- 2005:2009

# fixed getRouteData
RegionsForZipFiles <- GetRegions(ZipFiles = TRUE)
GetRouteData <- function(AOU=NULL, countrynum=NULL, states=NULL, year, weather=NULL, routes=NULL, 
                         Zeroes=TRUE, TenStops = FALSE, 
                         Dir="ftp://ftpext.usgs.gov/pub/er/md/laurel/BBS/DataFiles/") {
  
  if(TenStops) {
    DirData <- paste0(Dir, "States/")
    CountString <- "^count"
  } else {
    if(any(year<1997)) stop("Data only available from 1997: pre-1997 data not integrated into this function for 50 stop data (yet)")
    DirData <- paste0(Dir, "50-StopData/1997ToPresent_SurveyWide/")
    CountString <- "^stop"
  }
  if(!is.null(countrynum) & any(!(countrynum%in%c(124, 484, 840)))) stop("countrynum should be either 124 (Canada), 484 (Mexico), or 840 (USA)")
  
  GetDat <- function(file, dir, year, AOU, countrynum, states) {
    dat <- GetUnzip(ZipName=paste0(dir, file), FileName=gsub("^Fifty", "fifty", gsub("zip", "csv", file)))
    names(dat) <- tolower(names(dat))
    if(is.null(year)) {  UseYear <- TRUE  } else {  UseYear <- dat$year%in%year  }
    if(is.null(AOU)) {  UseAOU <- TRUE  } else {  UseAOU <- dat$aou%in%AOU  }
    if(is.null(countrynum)) {  UseCountry <- TRUE  } else {  UseCountry <- dat$countrynum%in%countrynum  }
    if(is.null(states)) {  UseState <- TRUE  } else {  UseState <- dat$statenum%in%states  }
    Use <- UseYear & UseAOU & UseCountry & UseState
    if(sum(Use)>0) {
      dat$routeID <- paste(dat$statenum, dat[,grep("^[Rr]oute$", names(dat))])
      dat <- subset(dat, subset=Use)
      return(dat)      
    } else return(NULL)
  }
  
  # Only use the files we want
  CountriesToUse <- if(!is.null(countrynum)) {
    RegionsForZipFiles$countrynum%in%countrynum 
  } else {
    TRUE
  }
  StatesToUse <- if(!is.null(states)) {
    RegionsForZipFiles$RegionCode%in%states 
  } else {
    TRUE
  }
  ToUse <- CountriesToUse & StatesToUse
  if(TenStops) {
    Files <- RegionsForZipFiles$FileName10stop[ToUse]
    Missing <- ToUse & is.na(RegionsForZipFiles$FileName10stop)
  } else { # 50 stop
    Files <- RegionsForZipFiles$FileName50stop[ToUse]
    Missing <- ToUse & is.na(RegionsForZipFiles$FileName50stop)
  }
  
  if(length(Files)==0) stop("No data for the states specified")
  if(any(is.na(Files))) warning(paste0("No data for these states: ", paste(RegionsForZipFiles$'State/Prov/TerrName'[Missing], collapse=", ")))
  
  Data.lst <- sapply(Files[!is.na(Files)], GetDat, dir=DirData, year=year, AOU=AOU, countrynum=countrynum, states=states, simplify=FALSE)
  
  if(all(unlist(lapply(Data.lst, is.null)))) {
    warning("no data, sorry")
    AllData <- NULL
  } else {
    Data <- ldply(Data.lst)
    # Get route data for all routes, and annual data
    if(is.null(weather)) weather <-GetWeather(Dir)
    if(is.null(year)) {  UseYear <- TRUE  } else {  UseYear <- weather$Year%in%year  }
    if(is.null(countrynum)) {  UseCountry <- TRUE  } else {  UseCountry <- weather$CountryNum%in%countrynum  }
    if(is.null(states)) {  UseState <- TRUE  } else {  UseState <- weather$StateNum%in%states  }
    UseWeather <- UseYear & UseCountry & UseState
    
    if(is.null(routes)) routes <- GetRoutes(Dir)
    if(is.null(countrynum)) {  UseCountry <- TRUE  } else {  UseCountry <- routes$CountryNum%in%countrynum  }
    if(is.null(states)) {  UseState <- TRUE  } else {  UseState <- routes$StateNum%in%states  }
    UseRoutes <- UseCountry & UseState
    
    CommonNames <- names(Data)[names(Data)%in%names(weather)]
    CommonNames <- CommonNames[CommonNames%in%names(routes)]
    
    # Subset data
    # First, sites sampled in chosen year(s)
    weather <-subset(weather, subset=UseWeather, 
                     select=c(CommonNames, "Year", "Month", "Day", "RunType", "StateNum"))
    # Route data for sites sampled in chosen years
    routes <- subset(routes, subset=UseRoutes & routes$routeID%in%weather$routeID, 
                     select=c(CommonNames, "Latitude", "Longitude", "StateNum"))
    
    # merge data sets
    dat.routeID.year <- paste(Data$routeID, Data$year, sep=".")
    routes$routeID <- paste0(routes$StateNum, routes$routeID)
    weather.routeID.year <- paste(paste0(weather$StateNum, weather$routeID), weather$Year, sep=".")
    
    WeatherWhiches <- match(dat.routeID.year, weather.routeID.year)
    
    RouteWhiches <- match(Data$routeID, routes$routeID)
    
    AllData <- cbind(Data, weather[WeatherWhiches, !names(weather)%in%names(Data)],
                     routes[RouteWhiches, !names(routes)%in%names(Data)])
    
    #  if(!is.na(weather)) AllData <- merge(Data, weather, all=TRUE) # by=c("routeID", "RPID"), 
    #  if(!is.na(routes))  AllData <- merge(AllData, routes, all=TRUE) # by="routeID", 
    AllData$SumCount <- apply(AllData[,grep(CountString, names(AllData))],1,sum, na.rm=TRUE)
    if(!Zeroes) AllData <- subset(AllData, AllData$SumCount>0)
    AllData <- AllData[,!names(AllData)%in%c(".id", "routedataid", "year")]
  }
  
  AllData
}


BBS_Wren <- GetRouteData(AOU=6540, countrynum = 840, states = PACode, year = PAYears,
                      weather = WeatherMetaData, routes = RoutesMetaData, TenStops = FALSE,
                      Zeroes = TRUE)

# counts are made along a route
# need to be made into number of presences and number of trials (routes)
BBS_Wren <- BBS_Wren %>%
  mutate(NPres = rowSums(dplyr::select(., starts_with("stop")) > 0)) %>%
  mutate(Ntrials = rowSums(!is.na(dplyr::select(., starts_with("stop"))))) %>%
  group_by(route) %>%
  summarise(Ntrials = sum(Ntrials),
            NPres = sum(NPres),
            Latitude = first(Latitude),
            Longitude = first(Longitude)) %>%
  rename(X = Longitude, Y = Latitude)

# change to spatial points
BBS_Wren <- as.data.frame(BBS_Wren)

BBS_sp <- SpatialPointsDataFrame(BBS_Wren[,c("X", "Y")],
                               data = BBS_Wren[,c("NPres", "Ntrials")],
                               proj4string = crs(proj))



# eBird / GBIF
# get GBIF data, filter for 2005-2009
GBIF <- read_delim("GBIF.csv", "\t", escape_double = FALSE, 
                   trim_ws = TRUE)
GBIF <- GBIF %>% 
  dplyr::select(decimalLatitude, decimalLongitude, year) %>%
  filter(year %in% c(2005:2009)) %>%
  rename(X = decimalLongitude, Y = decimalLatitude)

# make into spatial points
GBIF_coords <- cbind(GBIF$X, GBIF$Y)
GBIF_pts <- SpatialPoints(coords = GBIF_coords, proj4string = proj)
# trim to keep only those occuring in PA (with probably unnecessary back-and-forth of data formats)
GBIF_pts <- over(GBIF_pts, PA)
GBIF_pts <- data.frame(GBIF_coords[!is.na(GBIF_pts),])
GBIF_sp <- SpatialPoints(coords = GBIF_pts, proj4string = proj)
```

```{r data gathering - SUBSET}
proj <- CRS("+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0")

# BBA from Miller et al. appendix
load("PA_BBA.RData")

# sum counts (counts are undertaken in 5 time intervals at a single spatial point)
BBA_Wren <- bba %>%
  mutate(total = dplyr::select(., v1:v5) %>% rowSums(na.rm = TRUE)) %>%
  dplyr::select(-c(v1:v5)) %>%
  mutate(present = if_else(total == 0, FALSE, TRUE)) %>%
  rename(X = Longitude, Y = Latitude)

# change to spatial dataframe
BBA_sp <- SpatialPointsDataFrame(BBA_Wren[,c("X", "Y")],
                               data = BBA_Wren[,c("present", "point")],
                               proj4string = crs(proj))

# region around pts for subset
pnts <- BBA_Wren %>%
  st_as_sf(coords = c("X", "Y"), crs = 4326)
PA_subset <- concaveman(pnts, concavity = 3)
PA_subset <- as(st_geometry(PA_subset), "Spatial")


# BBS (using rBBS package)
RegionMetaData <- GetRegions()
WeatherMetaData <- GetWeather()
# GetRoutes function has an issue. It calls Routes.zip instead of routes.zip
GetRoutes <- function(Dir="ftp://ftpext.usgs.gov/pub/er/md/laurel/BBS/DataFiles/") {
  routes=GetUnzip(ZipName=paste0(Dir, "routes.zip"), FileName="routes.csv")
  routes$routeID=paste(routes$statenum, routes$Route)
  routes
}
RoutesMetaData <- GetRoutes()

PACode <- RegionMetaData$RegionCode[RegionMetaData$`State/Prov/TerrName` == "PENNSYLVANIA"]
PAYears <- 2005:2009

# fixed getRouteData
RegionsForZipFiles <- GetRegions(ZipFiles = TRUE)
GetRouteData <- function(AOU=NULL, countrynum=NULL, states=NULL, year, weather=NULL, routes=NULL, 
                         Zeroes=TRUE, TenStops = FALSE, 
                         Dir="ftp://ftpext.usgs.gov/pub/er/md/laurel/BBS/DataFiles/") {
  
  if(TenStops) {
    DirData <- paste0(Dir, "States/")
    CountString <- "^count"
  } else {
    if(any(year<1997)) stop("Data only available from 1997: pre-1997 data not integrated into this function for 50 stop data (yet)")
    DirData <- paste0(Dir, "50-StopData/1997ToPresent_SurveyWide/")
    CountString <- "^stop"
  }
  if(!is.null(countrynum) & any(!(countrynum%in%c(124, 484, 840)))) stop("countrynum should be either 124 (Canada), 484 (Mexico), or 840 (USA)")
  
  GetDat <- function(file, dir, year, AOU, countrynum, states) {
    dat <- GetUnzip(ZipName=paste0(dir, file), FileName=gsub("^Fifty", "fifty", gsub("zip", "csv", file)))
    names(dat) <- tolower(names(dat))
    if(is.null(year)) {  UseYear <- TRUE  } else {  UseYear <- dat$year%in%year  }
    if(is.null(AOU)) {  UseAOU <- TRUE  } else {  UseAOU <- dat$aou%in%AOU  }
    if(is.null(countrynum)) {  UseCountry <- TRUE  } else {  UseCountry <- dat$countrynum%in%countrynum  }
    if(is.null(states)) {  UseState <- TRUE  } else {  UseState <- dat$statenum%in%states  }
    Use <- UseYear & UseAOU & UseCountry & UseState
    if(sum(Use)>0) {
      dat$routeID <- paste(dat$statenum, dat[,grep("^[Rr]oute$", names(dat))])
      dat <- subset(dat, subset=Use)
      return(dat)      
    } else return(NULL)
  }
  
  # Only use the files we want
  CountriesToUse <- if(!is.null(countrynum)) {
    RegionsForZipFiles$countrynum%in%countrynum 
  } else {
    TRUE
  }
  StatesToUse <- if(!is.null(states)) {
    RegionsForZipFiles$RegionCode%in%states 
  } else {
    TRUE
  }
  ToUse <- CountriesToUse & StatesToUse
  if(TenStops) {
    Files <- RegionsForZipFiles$FileName10stop[ToUse]
    Missing <- ToUse & is.na(RegionsForZipFiles$FileName10stop)
  } else { # 50 stop
    Files <- RegionsForZipFiles$FileName50stop[ToUse]
    Missing <- ToUse & is.na(RegionsForZipFiles$FileName50stop)
  }
  
  if(length(Files)==0) stop("No data for the states specified")
  if(any(is.na(Files))) warning(paste0("No data for these states: ", paste(RegionsForZipFiles$'State/Prov/TerrName'[Missing], collapse=", ")))
  
  Data.lst <- sapply(Files[!is.na(Files)], GetDat, dir=DirData, year=year, AOU=AOU, countrynum=countrynum, states=states, simplify=FALSE)
  
  if(all(unlist(lapply(Data.lst, is.null)))) {
    warning("no data, sorry")
    AllData <- NULL
  } else {
    Data <- ldply(Data.lst)
    # Get route data for all routes, and annual data
    if(is.null(weather)) weather <-GetWeather(Dir)
    if(is.null(year)) {  UseYear <- TRUE  } else {  UseYear <- weather$Year%in%year  }
    if(is.null(countrynum)) {  UseCountry <- TRUE  } else {  UseCountry <- weather$CountryNum%in%countrynum  }
    if(is.null(states)) {  UseState <- TRUE  } else {  UseState <- weather$StateNum%in%states  }
    UseWeather <- UseYear & UseCountry & UseState
    
    if(is.null(routes)) routes <- GetRoutes(Dir)
    if(is.null(countrynum)) {  UseCountry <- TRUE  } else {  UseCountry <- routes$CountryNum%in%countrynum  }
    if(is.null(states)) {  UseState <- TRUE  } else {  UseState <- routes$StateNum%in%states  }
    UseRoutes <- UseCountry & UseState
    
    CommonNames <- names(Data)[names(Data)%in%names(weather)]
    CommonNames <- CommonNames[CommonNames%in%names(routes)]
    
    # Subset data
    # First, sites sampled in chosen year(s)
    weather <-subset(weather, subset=UseWeather, 
                     select=c(CommonNames, "Year", "Month", "Day", "RunType", "StateNum"))
    # Route data for sites sampled in chosen years
    routes <- subset(routes, subset=UseRoutes & routes$routeID%in%weather$routeID, 
                     select=c(CommonNames, "Latitude", "Longitude", "StateNum"))
    
    # merge data sets
    dat.routeID.year <- paste(Data$routeID, Data$year, sep=".")
    routes$routeID <- paste0(routes$StateNum, routes$routeID)
    weather.routeID.year <- paste(paste0(weather$StateNum, weather$routeID), weather$Year, sep=".")
    
    WeatherWhiches <- match(dat.routeID.year, weather.routeID.year)
    
    RouteWhiches <- match(Data$routeID, routes$routeID)
    
    AllData <- cbind(Data, weather[WeatherWhiches, !names(weather)%in%names(Data)],
                     routes[RouteWhiches, !names(routes)%in%names(Data)])
    
    #  if(!is.na(weather)) AllData <- merge(Data, weather, all=TRUE) # by=c("routeID", "RPID"), 
    #  if(!is.na(routes))  AllData <- merge(AllData, routes, all=TRUE) # by="routeID", 
    AllData$SumCount <- apply(AllData[,grep(CountString, names(AllData))],1,sum, na.rm=TRUE)
    if(!Zeroes) AllData <- subset(AllData, AllData$SumCount>0)
    AllData <- AllData[,!names(AllData)%in%c(".id", "routedataid", "year")]
  }
  
  AllData
}


BBS_Wren <- GetRouteData(AOU=6540, countrynum = 840, states = PACode, year = PAYears,
                      weather = WeatherMetaData, routes = RoutesMetaData, TenStops = FALSE,
                      Zeroes = TRUE)

# counts are made along a route
# need to be made into number of presences and number of trials (routes)
BBS_Wren <- BBS_Wren %>%
  mutate(NPres = rowSums(dplyr::select(., starts_with("stop")) > 0)) %>%
  mutate(Ntrials = rowSums(!is.na(dplyr::select(., starts_with("stop"))))) %>%
  group_by(route) %>%
  summarise(Ntrials = sum(Ntrials),
            NPres = sum(NPres),
            Latitude = first(Latitude),
            Longitude = first(Longitude)) %>%
  rename(X = Longitude, Y = Latitude)

# change to spatial points
BBS_Wren <- as.data.frame(BBS_Wren)

BBS_sp <- SpatialPointsDataFrame(BBS_Wren[,c("X", "Y")],
                               data = BBS_Wren[,c("NPres", "Ntrials")],
                               proj4string = crs(proj))

BBS_sp <- raster::crop(BBS_sp, extent(-78.4968, -76.5027, 40.5026, 41.4981))


# eBird / GBIF
# get GBIF data, filter for 2005-2009
GBIF <- read_delim("GBIF.csv", "\t", escape_double = FALSE, 
                   trim_ws = TRUE)
GBIF <- GBIF %>% 
  dplyr::select(decimalLatitude, decimalLongitude, year) %>%
  filter(year %in% c(2005:2009)) %>%
  rename(X = decimalLongitude, Y = decimalLatitude)

# make into spatial points
GBIF_coords <- cbind(GBIF$X, GBIF$Y)
GBIF_pts <- SpatialPoints(coords = GBIF_coords, proj4string = proj)
GBIF_sp <- raster::crop(GBIF_pts, extent(-78.4968, -76.5027, 40.5026, 41.4981))

```


```{r plot data}
cols <- c(brewer.pal(4, "Set1"))
names(cols) <- c("BBS", "GBIF", "BBA, absent", "BBA, present")
plot(PA)
points(BBS_sp, cex = 0.5, pch = 19, col = cols["BBS"])
points(GBIF_sp, cex = 0.5, pch = 19, col = cols["GBIF"])
points(BBA_sp, cex = 0.5, pch = 19, col = cols[c("BBA, absent", "BBA, present")][1+BBA_sp@data$present])
legend("topright", inset = c(-0.05,-0.05), xpd= TRUE, legend = names(cols), fill = cols, cex = 0.8)
```

## Covariates
```{r cov gathering}
# elevation data using elevatr (could theoretically also use FedData but get holes in elev raster)
elev <- get_elev_raster(PA, z = 8, clip = "locations") #z = 1 for lowest res, z = 14 for highest (DL time very long)
elevation <- as.data.frame(elev, xy = T, na.rm = T)

# canopy from the NLCD
NLCD_canopy <- get_nlcd(template = PA, year = 2011, dataset = "canopy", label = "PA_lc")
NLCD_canopy <- projectRaster(from = NLCD_canopy, to = elev)
NLCD_canopy <- mask(NLCD_canopy, elev)
canopy <- as.data.frame(NLCD_canopy, xy = T, na.rm = T)

covariates <- full_join(elevation, canopy, by = c("x", "y"))
covariates <- covariates %>%
  rename(elevation = layer, canopy = PA_lc_NLCD_2011_canopy, X = x, Y = y)
covariates <- SpatialPointsDataFrame(covariates[,c("X", "Y")], 
                                     data = covariates[,c("elevation", "canopy")], proj4string = crs(proj))
covariates@data <-data.frame(apply(covariates@data, 2 , scale))  # scale the covariates

# population data from US 2010 census
# Function to add population data (from 2010 census) to a data frame # Arguments
#   data: spatial points data frame to add the data to
#   proj: Projection (CRS()))
#   censuskey: Census key to get population data.
Add2010Census <- function(data, proj, censuskey) {
   require(censusapi)
   require(maps)
   require(maptools)
   require(rgeos)
# Decennial Census sf3, 2010
#  censuskey <- ....
   # get population data
   data2010 <- getCensus(name="dec/sf1", vintage=2010, key=censuskey,
                         vars="P008001", region="county:*")
   rownames(data2010) <- paste0(data2010$state, data2010$county)

   # Get county maps, and make into spatial polygon
   county <- map('county', fill=TRUE)
   data(county.fips)
   county.fips$fips <- as.character(county.fips$fips)
   county.fips$fips[nchar(county.fips$fips)==4] <- paste0("0",
county.fips$fips[nchar(county.fips$fips)==4])

   county$FIPS <- county.fips$fips[match(map("county",
plot=FALSE)$names, county.fips$polyname)]
   CountyMap <- map2SpatialPolygons(map=county, IDs = county$FIPS, proj4string = proj)
   Countydat <- data.frame(area = unlist(lapply(CountyMap@polygons,
function(ply) ply@area)),
                           FIPS = unlist(lapply(CountyMap@polygons,
function(ply) ply@ID)),
                           row.names = unlist(lapply(CountyMap@polygons,
function(ply) ply@ID)),
                           stringsAsFactors = FALSE)

# Merge population and county data
   Countydat.m <- merge(Countydat, data2010, by="row.names")

   rownames(Countydat.m) <- rownames(Countydat)
   Countydat.m$density <- Countydat.m$P008001/Countydat.m$area
   CountyPops <- SpatialPolygonsDataFrame(data=Countydat.m, Sr=CountyMap)

# extract data from correct polygon for data, and merge
   PopData <- over(x=data, y=CountyPops)
   data@data <- cbind(data@data, PopData[,c("FIPS", "density")])
   data
}

censuskey <- "ba4f7dd49b22e58b5e6a5cc99b349b555bbf95a8"
covariates_gbif <- Add2010Census(covariates, proj, censuskey)
covariates_gbif@data$FIPS <- as.numeric(covariates_gbif@data$FIPS)
covariates_gbif@data <-data.frame(apply(covariates_gbif@data, 2 , scale))


```

## Data preparation
The model works by approximating the continuous space by a tesselation of triangles, so we have to create that tesselation, i.e. the mesh.
```{r MakeMesh}
# just took PointedSDM's function apart a bit because buffer function used in MakeSpatialRegion doesn't work with unprojected data - so no buffering here
MakeSpatialRegion2 <- function (data = NULL, coords = c("X", "Y"), meshpars, bdry = NULL, 
    proj = CRS("+proj=utm")) 
{    region.bdry <- inla.sp2segment(bdry)
    mesh <- inla.mesh.2d(boundary = region.bdry, cutoff = meshpars$cutoff, 
        max.edge = meshpars$max.edge, offset = meshpars$offset)
    spde <- inla.spde2.matern(mesh = mesh, alpha = 2)
    dd <- deldir::deldir(mesh$loc[, 1], mesh$loc[, 2])
    tiles <- deldir::tile.list(dd)
    poly.gpc <- as(bdry@polygons[[1]]@Polygons[[1]]@coords, "gpc.poly")
    w <- sapply(tiles, function(p) rgeos::area.poly(rgeos::intersect(as(cbind(p$x, 
        p$y), "gpc.poly"), poly.gpc)))
    return(list(mesh = mesh, spde = spde, w = w))
}

# change mesh vars with Meshpars -> more vertices = better approximation = longer calculations 
# units same as in projection
Meshpars <- list(max.edge = c(0.15, 0.3), offset = c(0.1, 0.4), cutoff = 0.2)

Mesh <- MakeSpatialRegion2(data = NULL, bdry = PA, meshpars = Meshpars,
                          proj =  proj)

stk.ip <- MakeIntegrationStack(mesh = Mesh$mesh, data = covariates, area=Mesh$w, 
                               tag='ip', InclCoords=TRUE)
```

```{r plot mesh}
plot(Mesh$mesh)
```

The blue line is our actual region. The outer line is a necessary expansion to avoid boundary effects.


Next, create some data that we want to predict onto, in order to draw a map of the predicted presence. The resolution of the map is controlled by Nxy.scale (the width and height in pixels).
```{r pred, eval = FALSE}
# make data for projections
Nxy.scale <- 0.01 # change the resolution of the predictions
Boundary <- Mesh$mesh$loc[Mesh$mesh$segm$int$idx[,2],]
Nxy <- round(c(diff(range(Boundary[,1])), diff(range(Boundary[,2])))/Nxy.scale)
stk.pred <- MakeProjectionGrid(nxy = Nxy, mesh = Mesh$mesh, data = covariates,
             tag = 'pred', boundary = Boundary)
```

Now format the data into 'stacks' to be used in the model.
```{r stacks}
# data stacks
stk.BBS <- MakeBinomStack(observs = BBS_sp,data = covariates, mesh=Mesh$mesh,
                          presname="NPres", trialname="Ntrials", tag='BBS', InclCoords=TRUE)
stk.gbif <- MakePointsStack(presences = GBIF_sp, data = covariates_gbif, mesh = Mesh$mesh, 
                            tag = 'gbif', InclCoords = TRUE)
stk.BBA <- MakeBinomStack(observs = BBA_sp, data = covariates, mesh = Mesh$mesh,
                            presname='present', tag='BBA', InclCoords=TRUE)
```

## Model and results

Fit the model.
```{r fit model}
# modified function to take PC priors
FitModel2 <- function (..., formula = NULL, CovNames = NULL, mesh, spat.ind = "i", 
          predictions = FALSE, tag.pred = "pred", control.fixed = NULL, 
          waic = FALSE, dic = FALSE) 
{
  stck <- inla.stack(stk.ip, stk.pred$stk, stk.gbif, stk.BBS, stk.BBA)
  if (is.null(CovNames)) {
    CovNames <- unlist(stck$effects$names)
    CovNames <- CovNames[!CovNames %in% c(spat.ind)]
  }
  else {
    if (!is.null(formula)) {
      warning("CovNames and formula are both not NULL: CovNames will be ignored")
    }
  }
  mesh <- inla.spde2.matern(mesh)
  if (!is.null(spat.ind)) {
    CovNames <- c(CovNames, paste0("f(", spat.ind, ", model=mesh)"))
  }
  if (is.null(control.fixed)) {
    control.fixed <- list(mean = 0)
  }
  if (is.null(formula)) {
    Formula <- formula(paste(c("resp ~ 0 ", CovNames), collapse = "+"))
  }
  else {
    if (is.null(spat.ind)) {
      Formula <- formula
    }
    else {
      if (any(grepl(paste0("(", spat.ind, ","), formula, 
                    fixed = TRUE))) {
        warning(paste0(spat.ind, " already in formula, so will be ignored"))
        Formula <- formula
      } else {
      Formula <- update(formula, paste0(" ~ . + f(", spat.ind, 
                                        ", model=mesh)"))
    }
    }
  }
  mod <- inla(Formula, family = c("poisson", "binomial"), control.family = list(list(link = "log"), 
                                                                                list(link = "cloglog")), data = inla.stack.data(stck), 
              verbose = FALSE, control.results = list(return.marginals.random = FALSE, 
                                                      return.marginals.predictor = FALSE), control.predictor = list(A = inla.stack.A(stck), 
                                                                                                                    link = NULL, compute = TRUE), control.fixed = control.fixed, 
              Ntrials = inla.stack.data(stck)$Ntrials, E = inla.stack.data(stck)$e, 
              control.compute = list(waic = waic, dic = dic))
  if (predictions) {
    id <- inla.stack.index(stck, tag.pred)$data
    pred <- data.frame(mean = mod$summary.fitted.values$mean[id], 
                       stddev = mod$summary.fitted.values$sd[id])
    res <- list(model = mod, predictions = pred)
  }
  else {
    res <- mod
  }
  res
}

# create priors 
C.F. <- list(mean = list(int.BBS = 0, int.gbif = 0, int.BBA = 0),
    mean.intercept=0, prec.intercept = 0.001,
    prec = list(int.BBS = 1, int.gbif = 1, int.BBA = 1))

# The parameters "prior.range" and "prior.sd" control the joint prior on range and standard deviation of the spatial field.
# see here for details: 
# https://groups.google.com/d/msg/r-inla-discussion-group/dunoXK_yAco/JhmYb5JoAQAJ
# e.g.: c(0.01, 0.05) means a 5% that the range will be less than 0.01
# specific notes: "it is advisable not to favour ranges that are smaller than the resolution of the mesh"
# "At this point I think one has to do some experimenting with the priors. As far as I know, we still do not have enough experience with the priors to come up with clearer guidelines"

spde <- inla.spde2.pcmatern(mesh = Mesh$mesh, alpha = 2, 
                             prior.range = c(0.02, 0.5),
                             prior.sigma = c(5, 0.1))

form <- formula(resp ~ 0 + elevation + canopy + Intercept + X + Y + int.BBS + 
                  int.gbif + int.BBA + f(i, model = spde))

# FitModel2 is a fixed version of Bob's FitModel
warbler_model <- FitModel2(stk.ip, stk.pred$stk, stk.gbif, stk.BBS, stk.BBA,
                                formula = form, CovNames = NULL, mesh = Mesh$mesh,
                                predictions = TRUE, control.fixed = C.F., waic = TRUE)
```

View outputs.
```{r outputs, message = FALSE}
warbler_model$model$summary.fixed
```

Prepare for interactive map plot.
```{r compplots, results = 'hide', warning = FALSE, message = FALSE}
Pred <- SpatialPixelsDataFrame(points = stk.pred$predcoords, data = warbler_model$predictions, proj4string = crs(proj))
Pred@data$precision <- Pred@data$stddev^-2
ncolours <- 200
meancols.fn <- colorRampPalette(brewer.pal(9, 'Oranges'))
meancols <- meancols.fn(ncolours)
map.mean <- mapview(Pred, zcol = c("mean"), legend = TRUE, 
                    col.regions=meancols, layer.name = "Mean pred")


sdcols.fn <- colorRampPalette(brewer.pal(9, 'Blues'))
sdcols <- sdcols.fn(ncolours) 
map.stddev <- mapview(Pred, zcol = c("stddev"), legend = TRUE, alpha=0.3, 
                      col.regions = sdcols, layer.name = "SD pred")


elevation <- covariates[,-(2)]
elev.fn <- colorRampPalette(brewer.pal(9, 'Spectral'))
elevcol <- elev.fn(ncolours)
map.elev <- mapview(elevation, legend = TRUE, col.regions = elevcol, layer.name = "Elevation")

canopy <- covariates[,-(1)]
can.fn <- colorRampPalette(brewer.pal(9, 'BuGn'))
cancol <- can.fn(ncolours)
map.can <- mapview(NLCD_canopy, legend = TRUE, col.regions = cancol, layer.name = "Canopy")


map.bbs <- mapview(BBS_sp, legend = TRUE, layer.name = "BBS", cex = 4, lwd = 0.5, col.regions = "yellow")
map.gbif <- mapview(GBIF_sp, legend = TRUE, layer.name = "GBIF", cex = 4, lwd = 0.5, col.regions = "pink")

BBAabs <- BBA_sp[which(BBA_sp@data$present == F),]
BBApres <- BBA_sp[which(BBA_sp@data$present == T),]
map.bba1 <- mapview(BBAabs, legend = TRUE, layer.name = "BBA absent", cex = 4, lwd = 0.5, col.regions = "grey")
map.bba2 <- mapview(BBApres, legend = TRUE, layer.name = "BBA present", cex = 4, lwd = 0.5, col.regions = "green")
```

Two interactive maps (crashes if all put into one). First data points, then covariates and predictions.

```{r plot1}
map.bbs + map.gbif + map.bba1 + map.bba2
```

```{r plot2}
map.mean + map.stddev + map.elev + map.can + map.bbs + map.gbif + map.bba1 + map.bba2
```